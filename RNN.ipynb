{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xarvel/DataScience/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxIvX3jnR-Ni",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "Mf9mXb2HQmBu"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_stats(text):\n",
        "  sample_size = 250\n",
        "  print(f'Sample {sample_size} characters:')\n",
        "  print('-' * 80)\n",
        "  # Take a look at the first 250 characters in text\n",
        "  print(text[:sample_size])\n",
        "  print('-' * 80)\n",
        "  # length of text is the number of characters in it\n",
        "  print(f'Length of text: {len(text)} characters')\n",
        "  # The unique characters in the file\n",
        "  vocab = sorted(set(text))\n",
        "  print(f'{len(vocab)} unique characters')\n",
        "\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "Vz9YuBUtXAZ0"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWjUhFzvUb1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bca709d-5ce4-4628-abdb-6c06c7d2d876",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = text_stats(text)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 250 characters:\n",
            "--------------------------------------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# The embedding dimension\n",
        "EMBEDDING_DIMENTION = 256\n",
        "\n",
        "# Number of RNN units\n",
        "RNN_UNITS = 1024"
      ],
      "metadata": {
        "id": "evYWmHCZ3ymu"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None\n",
        ")\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "x8LfVRWqyaoK"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "VOCAB_SIZE = len(ids_from_chars.get_vocabulary())"
      ],
      "metadata": {
        "id": "uPP-G3X4PrcT"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "Vo6-HzZkR1Pl"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "sequences = ids_dataset.batch(SEQUENCE_LENGTH + 1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "rj3-jNK5R49M"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNfX5YOhSFeN",
        "outputId": "992af27b-bdc3-48e4-8dce-8ef2774b7a1f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3075WT7nSGsV",
        "outputId": "235a739a-61ea-464d-d937-74f0f47502c9"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(\n",
        "        rnn_units,\n",
        "        return_sequences=True,\n",
        "        return_state=True\n",
        "    )\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "dlQT7LicSLl7"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIMENTION,\n",
        "    rnn_units=RNN_UNITS\n",
        ")"
      ],
      "metadata": {
        "id": "7tcX8ioUSMvD"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build((SEQUENCE_LENGTH, VOCAB_SIZE))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4_AVijRSQgr",
        "outputId": "50bc5393-d398-4ebf-e7b9-0a40fbebae40"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"rnn_model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_5 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_5 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "-mOgRkc0SU5y"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "FcOJJEurSY30"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "pUwfIDZfSaGS"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "eF-C8c-KSbLB"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHQ50UIScN-",
        "outputId": "30351763-feab-4323-fb6b-389e87dba8ab"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 13s 54ms/step - loss: 2.7033\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.9796\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.7053\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.5448\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.4480\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 13s 54ms/step - loss: 1.3811\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3289\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2850\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2455\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2066\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1673\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.1270\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 13s 55ms/step - loss: 1.0853\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.0398\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9933\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9430\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8930\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8405\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.7901\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "qCKnc8DESdgT"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "T6xQNohbSetS"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(start_text, size):\n",
        "  start = time.time()\n",
        "  states = None\n",
        "  next_char = tf.constant([start_text])\n",
        "  result = [next_char]\n",
        "\n",
        "  for n in range(size):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  end = time.time()\n",
        "  print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_' * 80)\n",
        "  print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "zmQH3Ip-ahiU"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_text('ROMEO:', 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzMMnyOKSgCz",
        "outputId": "b5d95ede-c06a-4236-e837-c137a7c4ee90"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Should well I wot.\n",
            "\n",
            "POLIXENES:\n",
            "I bring thee, nay, return; but that?\n",
            "\n",
            "Third Lord:\n",
            "This burn it off, and then I see the crown to blow our crown?\n",
            "This wash his leng his new done together: narry, come.\n",
            "\n",
            "Clown:\n",
            "You art good at an evect vault.\n",
            "\n",
            "KING RICHARD III:\n",
            "You speak blaspheal, that I have secure, sir.\n",
            "\n",
            "Citizens:\n",
            "Or else so happy maid away?\n",
            "\n",
            "BRUTUS:\n",
            "O, well and leap'd;\n",
            "Would I was even to be your conversage?\n",
            "Thy father was uneven a man that further like a winner?\n",
            "\n",
            "CORIOLANUS:\n",
            "Tut, thus far off in mine the lark too light! Was never traitor\n",
            "To cheer it to a question of my sorrow?\n",
            "Nay, then she is it will Forron, word she had never\n",
            "Be rather than it was, maligies than shame.\n",
            "To the fatuless hands of their death, my best for being Rusland's frozen:\n",
            "The wisdom friend, whiles who by his throne of marter day,\n",
            "That love the time to come o' the compessors.\n",
            "\n",
            "First Murderer:\n",
            "So I have bees deposed; the old wills time remove\n",
            "That thou hast spoke too entertainment,\n",
            "Not unlessordly he was nothing; a \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5295872688293457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXkZo_8ASjbU",
        "outputId": "10eee78d-8fd3-47d9-e9c4-b1880a9b129d"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7ac654db3220>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(RNNModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "W6gPOxecSl85"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIMENTION,\n",
        "    rnn_units=RNN_UNITS\n",
        ")"
      ],
      "metadata": {
        "id": "f0DVpo6VSnDI"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "uZSCd3euSopT"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMBf2oekSrI_",
        "outputId": "d2b9f173-201a-46fe-ea7e-9b89e469a065"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 56ms/step - loss: 2.6913\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ac635fe1a20>"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud0K8o2BStp_",
        "outputId": "87d44e3c-06de-425d-f82d-f79334777908"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1646\n",
            "Epoch 1 Batch 50 Loss 2.0442\n",
            "Epoch 1 Batch 100 Loss 1.9736\n",
            "Epoch 1 Batch 150 Loss 1.8291\n",
            "\n",
            "Epoch 1 Loss: 1.9718\n",
            "Time taken for 1 epoch 12.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7732\n",
            "Epoch 2 Batch 50 Loss 1.7346\n",
            "Epoch 2 Batch 100 Loss 1.6363\n",
            "Epoch 2 Batch 150 Loss 1.6587\n",
            "\n",
            "Epoch 2 Loss: 1.6926\n",
            "Time taken for 1 epoch 10.78 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5765\n",
            "Epoch 3 Batch 50 Loss 1.5295\n",
            "Epoch 3 Batch 100 Loss 1.4777\n",
            "Epoch 3 Batch 150 Loss 1.5122\n",
            "\n",
            "Epoch 3 Loss: 1.5354\n",
            "Time taken for 1 epoch 11.71 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4386\n",
            "Epoch 4 Batch 50 Loss 1.4380\n",
            "Epoch 4 Batch 100 Loss 1.4476\n",
            "Epoch 4 Batch 150 Loss 1.3873\n",
            "\n",
            "Epoch 4 Loss: 1.4378\n",
            "Time taken for 1 epoch 12.31 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3437\n",
            "Epoch 5 Batch 50 Loss 1.3504\n",
            "Epoch 5 Batch 100 Loss 1.3582\n",
            "Epoch 5 Batch 150 Loss 1.3105\n",
            "\n",
            "Epoch 5 Loss: 1.3713\n",
            "Time taken for 1 epoch 10.99 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3166\n",
            "Epoch 6 Batch 50 Loss 1.3378\n",
            "Epoch 6 Batch 100 Loss 1.3620\n",
            "Epoch 6 Batch 150 Loss 1.3235\n",
            "\n",
            "Epoch 6 Loss: 1.3184\n",
            "Time taken for 1 epoch 12.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2220\n",
            "Epoch 7 Batch 50 Loss 1.2297\n",
            "Epoch 7 Batch 100 Loss 1.2940\n",
            "Epoch 7 Batch 150 Loss 1.2826\n",
            "\n",
            "Epoch 7 Loss: 1.2737\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2096\n",
            "Epoch 8 Batch 50 Loss 1.2414\n",
            "Epoch 8 Batch 100 Loss 1.2087\n",
            "Epoch 8 Batch 150 Loss 1.2401\n",
            "\n",
            "Epoch 8 Loss: 1.2318\n",
            "Time taken for 1 epoch 11.39 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1715\n",
            "Epoch 9 Batch 50 Loss 1.2037\n",
            "Epoch 9 Batch 100 Loss 1.1953\n",
            "Epoch 9 Batch 150 Loss 1.2158\n",
            "\n",
            "Epoch 9 Loss: 1.1918\n",
            "Time taken for 1 epoch 10.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1065\n",
            "Epoch 10 Batch 50 Loss 1.1389\n",
            "Epoch 10 Batch 100 Loss 1.1343\n",
            "Epoch 10 Batch 150 Loss 1.1526\n",
            "\n",
            "Epoch 10 Loss: 1.1504\n",
            "Time taken for 1 epoch 11.10 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}