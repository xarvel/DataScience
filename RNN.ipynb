{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xarvel/DataScience/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxIvX3jnR-Ni",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "metadata": {
        "id": "Mf9mXb2HQmBu"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_stats(text):\n",
        "  sample_size = 250\n",
        "  print(f'Sample {sample_size} characters:')\n",
        "  print('-' * 80)\n",
        "  # Take a look at the first 250 characters in text\n",
        "  print(text[:sample_size])\n",
        "  print('-' * 80)\n",
        "  # length of text is the number of characters in it\n",
        "  print(f'Length of text: {len(text)} characters')\n",
        "  # The unique characters in the file\n",
        "  vocab = sorted(set(text))\n",
        "  print(f'{len(vocab)} unique characters')\n",
        "\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "Vz9YuBUtXAZ0"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWjUhFzvUb1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775f9415-d438-4fe0-a193-10722a4027cd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = text_stats(text)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 250 characters:\n",
            "--------------------------------------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itfRReywSFl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "17640495-082d-4780-a0b3-254528cfff52"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "Kt_44-ZhRsjf"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmzEDglERt0R",
        "outputId": "f802a54d-8258-4ad3-df48-113082b8d6e2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "C9JPPzKeRvcq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ipDFjtqRyJO",
        "outputId": "8cdfb0a8-16bf-469b-c9f7-b1c2b795b801"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d004N3BqRz5T",
        "outputId": "4fb54500-0358-499f-e4a8-fc9a5aeb5a9d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "Vo6-HzZkR1Pl"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjvWorFFR3n_",
        "outputId": "483b93ec-c0c0-4cc7-aef0-a80087ada1b6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "rj3-jNK5R49M"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieBlpPWjR6G7",
        "outputId": "e6fc79fc-3af4-4464-8a5e-42c1a471a9cb"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "83VeIpskR82K"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdxbw8NKR-LJ",
        "outputId": "d8fdf9fd-0bfe-4fcd-9004-7679f40374d3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HNtDteYR_pF",
        "outputId": "914c8a1c-d4a4-4130-96c0-6fe5ef387852"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "EKqkXTt9SBwp"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjDV_O2XSDEF",
        "outputId": "f2ad8abe-e124-4426-cd29-c5b17ecc03cb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "D7COmij_SEM7"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNfX5YOhSFeN",
        "outputId": "0aae0751-4cbb-45a8-a3a0-6a7236aecdcd"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3075WT7nSGsV",
        "outputId": "4ab905e5-f0f7-44ba-d516-5c1cf9783586"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "H2LtDu6DSJQQ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "dlQT7LicSLl7"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "metadata": {
        "id": "7tcX8ioUSMvD"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za-bLEpBSOQ7",
        "outputId": "a9700949-973a-411f-824f-00aae8a12f56"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4_AVijRSQgr",
        "outputId": "b887c6fc-d996-4a9b-d55a-ade1a880df4b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "wI4-kDZNSRiu"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGXMV1rbSSqj",
        "outputId": "98483549-3170-407f-d32d-093105edad71"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([27,  1, 61, 52, 41, 53, 26, 49,  2, 11, 17, 60,  5, 30, 59, 10, 40,\n",
              "       10, 19,  8, 65, 52, 27, 33, 52, 41, 29,  7, 55, 54,  4, 19, 62, 33,\n",
              "       17,  9, 63, 64,  9, 50, 64, 62, 24, 17, 63,  1, 27,  1, 27, 56, 32,\n",
              "       59, 40, 35, 18, 16, 63, 36, 49, 61, 65, 14, 14, 13,  1, 22, 41, 26,\n",
              "        9, 58, 30, 35, 52, 24, 59, 62, 42, 38, 62, 23, 21, 62, 41, 65, 59,\n",
              "        5, 43,  2, 42, 53, 15, 23, 35, 54, 40, 24, 32, 27, 33, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFt4AcIKST1P",
        "outputId": "3d659046-852c-44c5-9787-e4a436a7224c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'K:\\nAbout what?\\n\\nRICHARD:\\nAbout that which concerns your grace and us;\\nThe crown of England, father, '\n",
            "\n",
            "Next Char Predictions:\n",
            " b'N\\nvmbnMj :Du&Qt3a3F-zmNTmbP,po$FwTD.xy.kywKDx\\nN\\nNqStaVECxWjvzAA?\\nIbM.sQVmKtwcYwJHwbzt&d cnBJVoaKSNTS'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "-mOgRkc0SU5y"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chh_16zkSWEu",
        "outputId": "663d3c6b-82b1-4dff-960b-b9d30060bdff"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190171, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpbFkqajSXR3",
        "outputId": "90dfb28c-b920-4051-d167-13576c5d5648"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.034065"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "FcOJJEurSY30"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "pUwfIDZfSaGS"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "eF-C8c-KSbLB"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHQ50UIScN-",
        "outputId": "b4ab2ec8-037c-4a1f-b220-6ddf389d7b7a"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 61ms/step - loss: 2.7267\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.9970\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.7177\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.5536\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.4543\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3864\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3341\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2900\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.2497\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.2109\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1734\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1333\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0925\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0477\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0018\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9516\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 0.8999\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.8494\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7963\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.7467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "qCKnc8DESdgT"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "T6xQNohbSetS"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(start_text, size):\n",
        "  start = time.time()\n",
        "  states = None\n",
        "  next_char = tf.constant([start_text])\n",
        "  result = [next_char]\n",
        "\n",
        "  for n in range(size):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  end = time.time()\n",
        "  print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_' * 80)\n",
        "  print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "zmQH3Ip-ahiU"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_text('ROMEO:', 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzMMnyOKSgCz",
        "outputId": "a715f551-0e50-4cd7-828a-a56320573e0d"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Two I chappel that madam, you not: a great man\n",
            "what I promise you?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Romeo, there are too sore golden crowns. The\n",
            "sweeter Shot to do you go,\n",
            "A save forfeited and sweetly repair,\n",
            "Do evel to again for right,\n",
            "Have not done exactive,\n",
            "So saved the noble duke, unweighing\n",
            "gods, proud man than dost thou his shrunk.\n",
            "What you that kill'd her with a toath of tain,\n",
            "His honour and a closet-fulbert, I might\n",
            "Thus by false enly's and fortune,\n",
            "As if they were the life removed out of holy own.\n",
            "Ah, wouldding the wild and things and honour,\n",
            "Or your hearing still, and studience against somewhat\n",
            "to the law; the bridegroom in our intent\n",
            "Told ourselves that order glory. To him Dorpett\n",
            "Ir lowly asserve me? if she would prove it other,\n",
            "And all that love from ungraceful greatest enter\n",
            "Of tears are in respetite, and make\n",
            "her passing exercise, I am his,\n",
            "You must repart forth me to elsche yield\n",
            "As let it be true.\n",
            "\n",
            "VOLUMNIA:\n",
            "I new'd too wife.\n",
            "\n",
            "LUCENTIO:\n",
            "All man hear there, even of common grave;\n",
            "Of an \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.176940679550171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Q5jXd_ShVB",
        "outputId": "0e46477e-0632-41cd-c607-12d1e365533d"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThis rusty have I mean on their cheeks,\\nDid frame and vow'd down and so outray: may I had a\\ndesire to live, in Greece of unstain'd\\nsince Duke of York. Foult to himself and king!\\nFor Warwick well arrived out of my flesh.\\nBe pleased, tremble--as I did lip or\\nAngel: for a gloring am at Pomfret.\\n\\nVIRGILIA:\\nO, he is gone.\\n\\nLEONTES:\\nThou art neighbours:\\nThe Earl of Warwick will disloy him they house;\\nFor, knock joy; then among these woes unto me\\nAs is a minute that goveth since\\nProve in scorn at once: thrice noble mounh\\nTo wedded the air way shame to keep hildly be.\\n\\nPERDITA:\\nNow See where Obbour issue.\\n\\nBUSHY:\\nMy light and the lists of your delights?\\nHere is a bod on that advised\\nIs as a punking heavy days\\nBy admoried their stolen or surlent,--\\nMore Pomfret, Catesby, what say it,\\nHortensio, that is gone, to prison,\\nNot first he crowns in a peinzer.\\n\\nHORTENSIO:\\nSome heavy leave and well.\\n\\nDUCHESS OF YORK:\\nA good morrow's name, I hope.\\nThen, taking thyself and make the king.\\n\\nGONZALO:\\nHe mas\"\n",
            " b\"ROMEO:\\nVery well: and welcome of Verongian's anger.\\nAnd, like an eagle-gall'd men to be bitter\\ndistingues in our royal preged out of it.\\nThou hast some better hand of discontent,\\nMuch of you to your turn, richest thought!--\\nAs it were deserved of foul wings,\\nOff with your hands,'days to my foul in each sequence;\\nWhich never dream on poldries and graves in mine.\\n\\nDUKE OFfARD:\\nGrandam, an't like you.\\n\\nLEONTES:\\nGreat Ambit!\\n\\nCORIOLANUS:\\nWhat touch he we speak 'Scapt?\\nAccording to the loss of royal person,--\\nAs if a man more foe that kill'd\\nWhich dissemples on thee and ours:\\nStrike muschesible: do, so I speak,\\nYou may send for men, to make the greater\\nperson for millian\\nperhaps, woe for English royal time!\\nRomeo! but I draw in peril opposite,\\nTo meet cabriage did complain my meling;\\nShe even for round him most obedien.\\nThat Angelo's thus? I hope to-night\\nHath plan'd the any tear of an else beloved.\\nEre he kneel'd Warwick so, of you\\nAs 'twaft bease thee would be owes?\\n\\nMENENIUS:\\nAt another woman \"\n",
            " b\"ROMEO:\\nAnd more, let me be patient. I am slain:\\nLo, call to fight with ignoble fully.\\n\\nBUCKINGHAM:\\nNow, for quckingham and years; even so\\n\\nMARIANA:\\nPeace and wits,\\nIf you me given his worthy deeds distille.\\n\\nKING EDWARD IV:\\nNow, by Saint Peter's Church, will give myself;\\nFor a treacherous I cannot, sir.\\n\\nVINCENTIO:\\nThis doth the king, why, nobly would he arm'd\\nTo bring a July to the Lord Angelo,\\nA smopt a thing to mock itself.\\nHadst thou not credil air? O mischance, rather,\\nProvokest me it to us.\\n\\nBENVOLIO:\\nAy, tir.\\n\\nAUTOLYCUS:\\nHelp, Jevelim, and yet so proud a dog!\\nThou know'st in Patua, he be not displease\\nthee: I bid thy father brands to fury:\\nNor for us to the gods, ambodition;\\nFor good hostess, Signior Gremio: sink, for\\nwronged love, I beseech you, wife and sound.\\n\\nThird Watchman:\\nBut, if the glear with her heart spraded by me!\\nYet, to grow officer: for this answer\\nWill wrink treacherous breath to like\\nNo better eyeming eyes of rest,\\nFor whosing thou acquaite the victory!\\nMost damned wh\"\n",
            " b\"ROMEO:\\nThat Edward is escapable.\\n\\nPERDITA:\\nUS Busht:\\nGod save you, Menour must do it.\\n\\nQUEEN ELIZABETH:\\nAnd tyralt lower, the boaring breasts. In yoke-health,\\nDisplease thee to my will.\\n\\nFLORIZEL:\\nOnly,\\nNay, that's a scorn!\\n\\nPRINCE:\\nSee that merry had not whisper to!\\n\\nPRINCE EDWARD:\\nMy lord, my master than drosperst, I must follow.\\n\\nSICINIUS:\\nWhy, so young somerwite,\\nThat might have kept that vistrone and your dust\\nIn the summer breathin stope of death;\\nAnd then, I mean thus friar: O, but do not\\nnoble do: and, more him: I am prepared\\nSo many enough and a glistering\\nshore; then cannot choose out of you.\\n\\nLUCIO:\\nWhy, I'll uster my cousin, so.\\nArt thou sure dead lipes of York pleased with bristle liege.\\n\\nKING LEWIS XI:\\nRomeo, no man at it appears; to let him so,\\nShe was not victors, blood and honour here,\\nTo mock ad menenied are born to supply the mark.\\nNow will I write like a tedone,\\nMy advancement summon and unrevorted\\nThat God from quarrelling and our queen:--\\nWere my life. Yet you must be s\"\n",
            " b\"ROMEO:\\nCome, follow; yet, I know you might know.\\n\\nSecond Huntsman:\\nMy king, the king!\\n\\nPETRUCHIO:\\nAy, with imperial your only.\\n\\nANTONIO:\\nAnd who ne'er marketh hither.\\n\\nPOMPEY:\\nI not have some with splinters of Berkeley,\\nHe should before that sword of me;\\nAnd much in brown Efol't: 'fore heavy locks.\\nWhat wouldst thou? speak a word: if you burch a happy malice\\nLike devievitanions and further.\\n\\nKING LEWIS XI:\\nWhat news abrove met, Awfuce? dear\\nHast thou pluck'd with your inventors o' the king,\\nAnd made the and is more than Juption steads\\nBoth in you to your fortune villain,\\nAs if they kiss thy names shall make enemnes,\\nI think that my defects King Edward's sovereign such.\\nAnd with 't a dishonour'd law, as morning:\\nCome, to the queen: Now, if thy will discreeming\\nThat Lord with 's rap's a charnech-bourn,\\nWere jounded to revenge on me: she is queen.\\nFarewell: I bid him more abstantly go.\\nGo, sir; I pill them walcome move:\\nAnd I unwrounded her wanting with it.\\n\\nPRISCE:\\nAnd who sets them! and meet \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.217740535736084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXkZo_8ASjbU",
        "outputId": "af4131d4-7e85-431f-ffaf-a81c635dac80"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x78ff06be19f0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elFpQNA7SksO",
        "outputId": "b125684b-0794-4891-aba0-f9f4f2b767c4"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The queen with her husband, for prattling friends\n",
            "Apong a temporaze over'd on my treacherous,\n",
            "But t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "W6gPOxecSl85"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "metadata": {
        "id": "f0DVpo6VSnDI"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "uZSCd3euSopT"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMBf2oekSrI_",
        "outputId": "7feabac2-7396-4f83-f25e-6e8fbed2fb05"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 58ms/step - loss: 2.7406\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78feeb6663b0>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud0K8o2BStp_",
        "outputId": "0bb78024-eef3-416f-8c7c-e5cade7143a6"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2014\n",
            "Epoch 1 Batch 50 Loss 2.0813\n",
            "Epoch 1 Batch 100 Loss 1.9534\n",
            "Epoch 1 Batch 150 Loss 1.8573\n",
            "\n",
            "Epoch 1 Loss: 2.0073\n",
            "Time taken for 1 epoch 13.15 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7937\n",
            "Epoch 2 Batch 50 Loss 1.7321\n",
            "Epoch 2 Batch 100 Loss 1.7420\n",
            "Epoch 2 Batch 150 Loss 1.6268\n",
            "\n",
            "Epoch 2 Loss: 1.7233\n",
            "Time taken for 1 epoch 12.02 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6268\n",
            "Epoch 3 Batch 50 Loss 1.5937\n",
            "Epoch 3 Batch 100 Loss 1.5560\n",
            "Epoch 3 Batch 150 Loss 1.5513\n",
            "\n",
            "Epoch 3 Loss: 1.5566\n",
            "Time taken for 1 epoch 11.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4845\n",
            "Epoch 4 Batch 50 Loss 1.4787\n",
            "Epoch 4 Batch 100 Loss 1.4579\n",
            "Epoch 4 Batch 150 Loss 1.4226\n",
            "\n",
            "Epoch 4 Loss: 1.4555\n",
            "Time taken for 1 epoch 11.50 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3757\n",
            "Epoch 5 Batch 50 Loss 1.4220\n",
            "Epoch 5 Batch 100 Loss 1.3612\n",
            "Epoch 5 Batch 150 Loss 1.3581\n",
            "\n",
            "Epoch 5 Loss: 1.3846\n",
            "Time taken for 1 epoch 11.38 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3285\n",
            "Epoch 6 Batch 50 Loss 1.3279\n",
            "Epoch 6 Batch 100 Loss 1.3303\n",
            "Epoch 6 Batch 150 Loss 1.3705\n",
            "\n",
            "Epoch 6 Loss: 1.3312\n",
            "Time taken for 1 epoch 11.24 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2915\n",
            "Epoch 7 Batch 50 Loss 1.2817\n",
            "Epoch 7 Batch 100 Loss 1.2440\n",
            "Epoch 7 Batch 150 Loss 1.2709\n",
            "\n",
            "Epoch 7 Loss: 1.2852\n",
            "Time taken for 1 epoch 11.48 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.1842\n",
            "Epoch 8 Batch 50 Loss 1.2431\n",
            "Epoch 8 Batch 100 Loss 1.2222\n",
            "Epoch 8 Batch 150 Loss 1.2370\n",
            "\n",
            "Epoch 8 Loss: 1.2445\n",
            "Time taken for 1 epoch 11.57 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1857\n",
            "Epoch 9 Batch 50 Loss 1.1622\n",
            "Epoch 9 Batch 100 Loss 1.2108\n",
            "Epoch 9 Batch 150 Loss 1.1893\n",
            "\n",
            "Epoch 9 Loss: 1.2045\n",
            "Time taken for 1 epoch 11.76 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1441\n",
            "Epoch 10 Batch 50 Loss 1.1346\n",
            "Epoch 10 Batch 100 Loss 1.1648\n",
            "Epoch 10 Batch 150 Loss 1.1759\n",
            "\n",
            "Epoch 10 Loss: 1.1648\n",
            "Time taken for 1 epoch 11.77 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}