{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xarvel/DataScience/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxIvX3jnR-Ni",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "metadata": {
        "id": "Mf9mXb2HQmBu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_stats(text):\n",
        "  # length of text is the number of characters in it\n",
        "  print(f'Length of text: {len(text)} characters')\n",
        "  # The unique characters in the file\n",
        "  vocab = sorted(set(text))\n",
        "  print(f'{len(vocab)} unique characters')\n",
        "\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "Vz9YuBUtXAZ0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWjUhFzvUb1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e99753e-d2ce-4e70-f079-f8a7ddea1ff2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = text_stats(text)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iSZNCjYTBA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467d4387-664e-490b-9b5d-0c79c62455ef",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itfRReywSFl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "fdd01293-c157-41f6-f24b-b675c03d4be6"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "Kt_44-ZhRsjf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmzEDglERt0R",
        "outputId": "5ace4a7d-2826-4be9-b509-81ffdfa202ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "C9JPPzKeRvcq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ipDFjtqRyJO",
        "outputId": "0183de81-04f2-48a8-dcb9-d14f01bb1bd7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d004N3BqRz5T",
        "outputId": "5569aefa-308d-463d-c9ec-879c26cba468"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "Vo6-HzZkR1Pl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjvWorFFR3n_",
        "outputId": "5211826f-628c-40b9-ddb8-bf70811cd3c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "rj3-jNK5R49M"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieBlpPWjR6G7",
        "outputId": "296346c4-a1e3-4b4c-b19d-f50c5eb35da5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "83VeIpskR82K"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdxbw8NKR-LJ",
        "outputId": "be73b409-8523-4197-b08f-5519016a0fe2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HNtDteYR_pF",
        "outputId": "4282082d-0d6a-412a-e21e-6f69decdfe6a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "EKqkXTt9SBwp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjDV_O2XSDEF",
        "outputId": "52a92fdc-5bf9-4eeb-9e5d-c5443fca3d34"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "D7COmij_SEM7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNfX5YOhSFeN",
        "outputId": "b0d2636c-861a-4b16-cf03-ecbc4ebe8c38"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3075WT7nSGsV",
        "outputId": "cd02c7f7-b3ed-4aac-a232-b3dafbd550ce"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "H2LtDu6DSJQQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "dlQT7LicSLl7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "metadata": {
        "id": "7tcX8ioUSMvD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za-bLEpBSOQ7",
        "outputId": "966f3b93-0ad7-49d7-daac-fec0756a4a64"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4_AVijRSQgr",
        "outputId": "52100ab9-83e7-4dda-aab9-c24d349741e9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "wI4-kDZNSRiu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGXMV1rbSSqj",
        "outputId": "9cbbb35b-1d9e-419b-af95-5c0c8f0f68ed"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17, 55, 30, 48,  9, 51,  6, 34, 15, 24, 32, 31, 54, 46, 36, 16,  5,\n",
              "        6, 64,  1, 30, 57, 29, 45, 61, 27, 62, 53,  0, 26, 26, 53, 35, 55,\n",
              "       58, 62, 42, 26, 34, 61, 22, 49, 36,  8, 63, 23, 38,  3, 11, 29,  5,\n",
              "       41, 21, 30,  6, 27, 34, 11, 56, 32, 14, 28, 53,  3, 56, 24, 51, 21,\n",
              "       33, 11, 47, 19, 49, 61, 18, 50, 52, 45,  7, 63, 38, 47, 52, 24,  7,\n",
              "       57, 18, 16, 16, 12, 45, 27,  4, 26, 19, 44,  6, 30, 42, 57])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFt4AcIKST1P",
        "outputId": "f3c3671e-5068-4da6-a830-5fc3619edde2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'She embraces him.\\n\\nCAMILLO:\\nShe hangs about his neck:\\nIf she pertain to life let her speak too.\\n\\nPOL'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"DpQi.l'UBKSRogWC&'y\\nQrPfvNwn[UNK]MMnVpswcMUvIjW-xJY!:P&bHQ'NU:qSAOn!qKlHT:hFjvEkmf,xYhmK,rECC;fN$MFe'Qcr\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "-mOgRkc0SU5y"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chh_16zkSWEu",
        "outputId": "fd527885-2ca2-4f8c-b38d-7c1f94547238"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1910553, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpbFkqajSXR3",
        "outputId": "a0bc5c56-dc33-408c-c3e9-7a688392213d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.0925"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "FcOJJEurSY30"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "pUwfIDZfSaGS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "eF-C8c-KSbLB"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHQ50UIScN-",
        "outputId": "d89eb61a-3e8c-4707-ecdc-aefc614d6044"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 60ms/step - loss: 2.7266\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.9914\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.7138\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.5534\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.4540\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.3866\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3327\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2877\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2456\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2047\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1654\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1239\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0803\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.0336\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.9852\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9344\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8822\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8294\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7782\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "qCKnc8DESdgT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "T6xQNohbSetS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzMMnyOKSgCz",
        "outputId": "b9e5a409-d315-4c5f-b72a-4147227a969c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "They have been four-bledged tine: and there's the boy ornament:\n",
            "And let their demand post thou no others\n",
            "To the madam. Honest upper me far further,\n",
            "Instinging pity dignity: he smears him back again.\n",
            "\n",
            "KING RICHARD III:\n",
            "How now, charged me, boy!\n",
            "\n",
            "Messenger:\n",
            "My gracious up, Hastings and three.\n",
            "\n",
            "KING RICHARD III:\n",
            "Say that I say; bit it be there.\n",
            "\n",
            "BUCKINGHAM:\n",
            "How comes the minister nack, what lesson's members\n",
            "could have dones conceuns to die yet betime.\n",
            "\n",
            "DOKE OLBHARD:\n",
            "And to-morrow art thou now? this honour hath no hopeo's\n",
            "ratel guide their sure did living;\n",
            "And when he slain, whose public heart's endmance,\n",
            "And that stabs temples of this virtuous tribunes\n",
            "In those fruitful Loncoman to conceal our voices;\n",
            "Stand you thy wall.\n",
            "\n",
            "Citizens:\n",
            "These news are wormest, allow your hinder passage.\n",
            "\n",
            "KING RICHARD II:\n",
            "Mine ear I shall not be commanded for\n",
            "some powerful kinsman come; or if our grief hath\n",
            "Been poor breasty forgivence.\n",
            "\n",
            "YORK:\n",
            "Here comes the kingdom of peace but unstand.\n",
            "\n",
            "ROMEO:\n",
            "Am I now in my \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.213001012802124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Q5jXd_ShVB",
        "outputId": "dd57cc24-0afc-4851-f3d5-99030052108f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nShe speaks:\\nO, miserable my lord, gone to this end.\\n\\nKING EDWARD IV:\\nWhy, 'tis good neighbour, and, with my hand\\nHath held the fiftered to her friends.\\n\\nGONZALO:\\nHere, Hortensio.\\n\\nBoth\\nTribunes: what news? what, Johe not? Shall we go?\\n\\nROMEO:\\nWhy, sir, what you have wont? Thy love did vasient; that we maid\\nprepared this small be provised prince, Henry's mejicians,\\nTake nothing or forbid: it was he wear their heat, in him\\nTo enter in the faults which I have done.\\nGo,--your Romeo will be manied to mock no other of curses;\\nOf whether like next buits with paintenance,\\nMore than the moveth state that spoils where he\\nWas this, to dispose the soldiers to the ground,\\nAnd chat the mouth of more than mistresse o'er his meed:\\nNot shunned the harmy stain'd about his sacred man,\\nWhich way, until thou been mile!\\nHis injuries shall persuade him, nor he, or grace sir:\\nI'll tell you where he should be much, but to conserve\\nand true misation; and, thou voices made to colour\\nWe come to run as war, yet r\"\n",
            " b\"ROMEO:\\nThis, what's the may before the tyrant!\\n\\nPAULINA:\\nAdd-tack'd forward!\\n\\nPEdRUCHIO:\\nMarry, well disposed, moved; I will dear him.\\n\\nDUKE OF AUMERLE:\\nI beseech you, hear me, my good lord, impossible\\nTo be lost as you might return from that\\nRegeal where you were dissever'd:--that I was hurt us,\\nI will be witness to be your queen,\\nWho mad must reign in name from the city,\\nNot any of them both divine in heaven!\\nWhat you have been some braved way to ruis with her?\\nThat I may curse the devil arm,\\nHer face of high and pamp-sharp,\\nAnd practise his exclaim. 'tis like your lordship,\\nTo dream upon him! how has my sigh? not you there speak.\\n\\nQUEEN MARGARET:\\nBut there we learn, his hands and shortly be\\nhind enfurely to his sons of beasts.\\n\\nKING RICHARD II:\\nThen ta'en hence to the Duke of Gloucester;\\nAnd that some hat that kill me with death,\\nEncounten'd by him over-my land.\\nO, thy behow your highness, whom alone,\\nMost dearly as ungetter do.\\n\\nJULIET:\\nThanks, minding! she durst not six years\\nTo barry m\"\n",
            " b\"ROMEO:\\nTo pardon me to come; how she had some cack:\\nFor you may make the canopy of her owns.\\nI tender you prepared at this holy through by my own,\\nFor mercy, for an embassage there:\\nO, then besires the northern enemies\\nWhere the infant floposition\\nAttend me in the tongue, still slain by some pition.\\nThe firman meealitat of parenty\\nconceting means do sweat birchlowerl;\\nWater was ever man's, his blood no sin,\\nOr Eatilan of his health doth act disdain with\\nher, and so chop in one slaughter-by horse.\\nWhat, go and widow, to the\\nWolse? hoo! how she had dined a hare,\\nand rove by me; on mine own good will of my speech.\\nIn that, will not be drunken with her? Duce\\nWithin the city were you like to wive.\\n\\nKING RICHARD III:\\nThe harlot ne'er shall say my such awake,\\nPleased the soldiers as they.\\n\\nRICHMOND:\\nWhen cheer? Go not so strike abase old quence,\\nAnd of their bending tears on blasts,\\nYou are thy certain drogst on't. The\\npennett needful isle,\\nThis treason served to be from threat\\nThat modest may grow\"\n",
            " b\"ROMEO:\\nHath he been in the law than farther to-morrow\\nPity in the bugger day,\\nShould not prove fruir maid; which pols water?\\n\\nSecond Messenger:\\nMy traffic to rejoice into the feast;\\nOr as esteeman, brother, to the crown;\\nAnd, when he does, then, in King Richard,\\nLike enough was to retire you weep. If ever the wolf comes!\\nO holy a slave, to them billowless man made thee\\nwith him to seek him not; for's passing heir,\\nTo call me have I muttand thy hands where\\nthey there no pontrument blunt beheld\\nBut to conquerous beacings to thee back.\\nThe tales upon the Lord Northumberland,\\nWhose seal is true, that I had been\\nThe hour might have no dignity; he's wondrous,\\nThe loss of the house of this good new kill'd.\\nCome, Kate, thieves, for it our commission lights and\\ngraced messages of some blanks,\\nbeing true bearied that captive forgeths me, how had we this\\nThat in a man for thee a just, yet good must to Pemare\\nHis semple brows him for the farthest eat o' the eye,\\nHave pitch'd ribborn by the enemy\\nThit lu\"\n",
            " b\"ROMEO:\\nSweet execute, that nothing but people\\nWith the hollow great Apolling from supply,\\nI spird'd a brave foul sore, that he shall not come to Po.\\n\\nKING RICHARD III:\\nYou needless my help in me, and go white sting?\\nFine shame will ride the county;' if thou hast\\nman! O woful summer tack; there are they bleed\\nTo make a schange; yet she shall be orderly?\\nAnd, he is found: he shall go to be great;\\nAnd if I cannot blow thy dangerous company:\\nWhen you have married; let him not hear himself.\\nThey, as thou liest, would have your husband's rage,\\nDivides on angels of the senate,'s lived,\\nHath shown this followers to conceal it;\\nSto thee the benefit of Bohemia,\\nAnd all the requesters;\\nAnd therefore 'tis been subbled violation.\\n\\nBUCKINGHAM:\\nHave done, sweet life, what says him not? But that I may be in any kinddom,\\nAnd spurn ut the man luckly, made his oath\\nOn him, bright fall to Richmond, you shall find\\nBy every soble blaze of false hooking presence,\\nIf he be married, nor he two could but speak with m\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.830955743789673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXkZo_8ASjbU",
        "outputId": "91174a3d-e7ca-432b-ea19-a73f99c4fb54"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x78ff2a99c7f0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elFpQNA7SksO",
        "outputId": "594485a6-5fe7-4450-9896-30261cfc41de"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The field trust forgeth thee on thy wildness,\n",
            "And where he chiefs, shame thy affair.\n",
            "\n",
            "MONSA:\n",
            "I memp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "W6gPOxecSl85"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "metadata": {
        "id": "f0DVpo6VSnDI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "uZSCd3euSopT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMBf2oekSrI_",
        "outputId": "277af451-6732-41db-d22b-dcdfb9161677"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 59ms/step - loss: 2.7044\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ff040de9b0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud0K8o2BStp_",
        "outputId": "a249c71c-7218-46ef-f590-dfbd34e25e50"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1511\n",
            "Epoch 1 Batch 50 Loss 2.0434\n",
            "Epoch 1 Batch 100 Loss 1.9218\n",
            "Epoch 1 Batch 150 Loss 1.8372\n",
            "\n",
            "Epoch 1 Loss: 1.9793\n",
            "Time taken for 1 epoch 13.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8276\n",
            "Epoch 2 Batch 50 Loss 1.7126\n",
            "Epoch 2 Batch 100 Loss 1.7073\n",
            "Epoch 2 Batch 150 Loss 1.6494\n",
            "\n",
            "Epoch 2 Loss: 1.7042\n",
            "Time taken for 1 epoch 12.20 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6035\n",
            "Epoch 3 Batch 50 Loss 1.5375\n",
            "Epoch 3 Batch 100 Loss 1.5374\n",
            "Epoch 3 Batch 150 Loss 1.4967\n",
            "\n",
            "Epoch 3 Loss: 1.5444\n",
            "Time taken for 1 epoch 12.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5260\n",
            "Epoch 4 Batch 50 Loss 1.4214\n",
            "Epoch 4 Batch 100 Loss 1.4348\n",
            "Epoch 4 Batch 150 Loss 1.4225\n",
            "\n",
            "Epoch 4 Loss: 1.4451\n",
            "Time taken for 1 epoch 12.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3596\n",
            "Epoch 5 Batch 50 Loss 1.3801\n",
            "Epoch 5 Batch 100 Loss 1.3632\n",
            "Epoch 5 Batch 150 Loss 1.3680\n",
            "\n",
            "Epoch 5 Loss: 1.3766\n",
            "Time taken for 1 epoch 11.48 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3042\n",
            "Epoch 6 Batch 50 Loss 1.3354\n",
            "Epoch 6 Batch 100 Loss 1.3243\n",
            "Epoch 6 Batch 150 Loss 1.3114\n",
            "\n",
            "Epoch 6 Loss: 1.3240\n",
            "Time taken for 1 epoch 11.34 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2704\n",
            "Epoch 7 Batch 50 Loss 1.2757\n",
            "Epoch 7 Batch 100 Loss 1.2923\n",
            "Epoch 7 Batch 150 Loss 1.2783\n",
            "\n",
            "Epoch 7 Loss: 1.2788\n",
            "Time taken for 1 epoch 11.41 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2568\n",
            "Epoch 8 Batch 50 Loss 1.2103\n",
            "Epoch 8 Batch 100 Loss 1.2435\n",
            "Epoch 8 Batch 150 Loss 1.2381\n",
            "\n",
            "Epoch 8 Loss: 1.2371\n",
            "Time taken for 1 epoch 11.52 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1741\n",
            "Epoch 9 Batch 50 Loss 1.2159\n",
            "Epoch 9 Batch 100 Loss 1.2017\n",
            "Epoch 9 Batch 150 Loss 1.1839\n",
            "\n",
            "Epoch 9 Loss: 1.1973\n",
            "Time taken for 1 epoch 11.66 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.0921\n",
            "Epoch 10 Batch 50 Loss 1.1812\n",
            "Epoch 10 Batch 100 Loss 1.1419\n",
            "Epoch 10 Batch 150 Loss 1.1553\n",
            "\n",
            "Epoch 10 Loss: 1.1569\n",
            "Time taken for 1 epoch 11.82 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}