{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xarvel/DataScience/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxIvX3jnR-Ni",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "Mf9mXb2HQmBu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_stats(text):\n",
        "  sample_size = 250\n",
        "  print(f'Sample {sample_size} characters:')\n",
        "  print('-' * 80)\n",
        "  # Take a look at the first 250 characters in text\n",
        "  print(text[:sample_size])\n",
        "  print('-' * 80)\n",
        "  # length of text is the number of characters in it\n",
        "  print(f'Length of text: {len(text)} characters')\n",
        "  # The unique characters in the file\n",
        "  vocab = sorted(set(text))\n",
        "  print(f'{len(vocab)} unique characters')\n",
        "\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "Vz9YuBUtXAZ0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWjUhFzvUb1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1cc3e75-6356-4725-f82c-09cc0c37f4ba",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = text_stats(text)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 250 characters:\n",
            "--------------------------------------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# The embedding dimension\n",
        "EMBEDDING_DIMENTION = 256\n",
        "\n",
        "# Number of RNN units\n",
        "RNN_UNITS = 1024"
      ],
      "metadata": {
        "id": "evYWmHCZ3ymu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None\n",
        ")\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "x8LfVRWqyaoK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "VOCAB_SIZE = len(ids_from_chars.get_vocabulary())"
      ],
      "metadata": {
        "id": "uPP-G3X4PrcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "Vo6-HzZkR1Pl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "sequences = ids_dataset.batch(SEQUENCE_LENGTH + 1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "rj3-jNK5R49M"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNfX5YOhSFeN",
        "outputId": "40a2ca31-0a04-4916-8e52-43a71614b9d5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3075WT7nSGsV",
        "outputId": "137e72f7-71f2-4c16-ed3a-e0b40db29cef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(\n",
        "        rnn_units,\n",
        "        return_sequences=True,\n",
        "        return_state=True\n",
        "    )\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "dlQT7LicSLl7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RnnModel(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIMENTION,\n",
        "    rnn_units=RNN_UNITS\n",
        ")"
      ],
      "metadata": {
        "id": "7tcX8ioUSMvD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za-bLEpBSOQ7",
        "outputId": "7f17dcf6-6fd7-492c-c9ea-e37613a070d7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4_AVijRSQgr",
        "outputId": "da47a1f9-5df7-440c-e160-ce4a3a9799b7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "wI4-kDZNSRiu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGXMV1rbSSqj",
        "outputId": "6e00db4b-40e7-498c-e3fd-a12156069117"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19, 57, 65, 53, 35, 45, 44,  9, 55,  0, 27, 40, 49,  3, 44, 18, 19,\n",
              "        2, 33, 26, 64, 27,  7, 44, 30, 13, 34, 21, 59, 53, 21, 52,  6, 62,\n",
              "       10, 11, 64, 35, 22, 43, 42, 32, 26, 57, 34, 50, 50,  2,  6,  3, 45,\n",
              "       47, 64, 28, 60, 12, 50, 62, 26, 10, 46, 25, 64, 42,  9,  2, 33, 19,\n",
              "       21, 42, 32,  8, 26, 62, 60, 57, 63, 56, 16, 38,  1, 59, 40, 26, 51,\n",
              "       32, 59, 47, 45, 45, 58,  6, 22,  3, 31, 12, 55, 34, 64, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFt4AcIKST1P",
        "outputId": "7ab9922e-c2f5-48dd-8143-84d9a132fa69"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"der ta'en for you;\\nWith all swift speed you must away to France.\\n\\nKING RICHARD II:\\nNorthumberland, t\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"FrznVfe.p[UNK]Naj!eEF TMyN,eQ?UHtnHm'w3:yVIdcSMrUkk '!fhyOu;kwM3gLyc. TFHcS-MwurxqCY\\ntaMlSthffs'I!R;pUy:\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "-mOgRkc0SU5y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chh_16zkSWEu",
        "outputId": "b71b597d-2330-4bb5-c95b-3e1294266d38"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1912036, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpbFkqajSXR3",
        "outputId": "6678f03d-044e-4fdf-9ba5-7691615b1bd3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.1023"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "FcOJJEurSY30"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "pUwfIDZfSaGS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "eF-C8c-KSbLB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHQ50UIScN-",
        "outputId": "1546a245-c8a3-4d34-a05b-954bb9e8ab2e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 54ms/step - loss: 2.7174\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.9869\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.7057\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.5456\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.4477\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 14s 56ms/step - loss: 1.3807\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3276\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.2819\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.2420\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2024\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.1628\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.1213\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.0791\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.0333\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9852\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9347\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8829\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8312\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7799\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "qCKnc8DESdgT"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "T6xQNohbSetS"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(start_text, size):\n",
        "  start = time.time()\n",
        "  states = None\n",
        "  next_char = tf.constant([start_text])\n",
        "  result = [next_char]\n",
        "\n",
        "  for n in range(size):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  end = time.time()\n",
        "  print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_' * 80)\n",
        "  print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "zmQH3Ip-ahiU"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_text('ROMEO:', 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzMMnyOKSgCz",
        "outputId": "c99bb180-8010-4693-a416-87b3eff67bee"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "\n",
            "HORTENSI:\n",
            "What, have thou made?\n",
            "\n",
            "GRUMIO:\n",
            "Let me see: yet you marge me the counterfeit\n",
            "That you be at askance.\n",
            "\n",
            "MENENIUS:\n",
            "Why, sir, I am a Roman, and still your hearts,\n",
            "That they must upper, wife, if he lives like in ill\n",
            "thing to vice the time begin once perform 'gainst to\n",
            "Be kept to be those at your husband.\n",
            "\n",
            "PRISPERO:\n",
            "If it be so, sir, I say!\n",
            "Be there a madman to this pleasure, but, you are\n",
            "the first waying angel on the people.\n",
            "\n",
            "First Citizen:\n",
            "To lose it is; for by his fellow spend;\n",
            "Which I'll be you, sir, unkindness people.\n",
            "\n",
            "Capston:\n",
            "Not after me, I bare action.\n",
            "\n",
            "GRUMIO:\n",
            "Why Ispead, withal marry it without asking\n",
            "How many war calls not thy followers fight,\n",
            "We'll hear it, and we will still we may they\n",
            "behind the gidst as much before us.\n",
            "\n",
            "MENENIUS:\n",
            "One of his head, thou must be past.\n",
            "\n",
            "CORIOLANUS:\n",
            "It is before the Duke of York is ruship;\n",
            "Nay, head you do, and to thyself are present\n",
            "doth buing the peace. I cannot deny:\n",
            "Be up-deep-taintenance.\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "Well said, mascense aga \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.158193588256836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXkZo_8ASjbU",
        "outputId": "f4526e72-5bc4-4e35-d21c-edcf70f2efff"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7a05a5c909a0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(RNNModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "W6gPOxecSl85"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIMENTION,\n",
        "    rnn_units=RNN_UNITS\n",
        ")"
      ],
      "metadata": {
        "id": "f0DVpo6VSnDI"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "uZSCd3euSopT"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMBf2oekSrI_",
        "outputId": "b493477b-5fc8-42d0-dfc7-82d3b80cebaf"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 61ms/step - loss: 2.7316\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a059b94fd00>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud0K8o2BStp_",
        "outputId": "224a024a-b09d-4ef5-d8b3-89f9719fe37b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2386\n",
            "Epoch 1 Batch 50 Loss 2.0880\n",
            "Epoch 1 Batch 100 Loss 1.9554\n",
            "Epoch 1 Batch 150 Loss 1.8521\n",
            "\n",
            "Epoch 1 Loss: 2.0028\n",
            "Time taken for 1 epoch 13.48 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7988\n",
            "Epoch 2 Batch 50 Loss 1.7886\n",
            "Epoch 2 Batch 100 Loss 1.7438\n",
            "Epoch 2 Batch 150 Loss 1.6415\n",
            "\n",
            "Epoch 2 Loss: 1.7254\n",
            "Time taken for 1 epoch 12.01 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6416\n",
            "Epoch 3 Batch 50 Loss 1.5681\n",
            "Epoch 3 Batch 100 Loss 1.5425\n",
            "Epoch 3 Batch 150 Loss 1.5271\n",
            "\n",
            "Epoch 3 Loss: 1.5615\n",
            "Time taken for 1 epoch 11.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4879\n",
            "Epoch 4 Batch 50 Loss 1.4781\n",
            "Epoch 4 Batch 100 Loss 1.4505\n",
            "Epoch 4 Batch 150 Loss 1.4478\n",
            "\n",
            "Epoch 4 Loss: 1.4590\n",
            "Time taken for 1 epoch 11.57 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3939\n",
            "Epoch 5 Batch 50 Loss 1.4508\n",
            "Epoch 5 Batch 100 Loss 1.4002\n",
            "Epoch 5 Batch 150 Loss 1.3652\n",
            "\n",
            "Epoch 5 Loss: 1.3892\n",
            "Time taken for 1 epoch 12.46 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3368\n",
            "Epoch 6 Batch 50 Loss 1.3355\n",
            "Epoch 6 Batch 100 Loss 1.3217\n",
            "Epoch 6 Batch 150 Loss 1.3897\n",
            "\n",
            "Epoch 6 Loss: 1.3360\n",
            "Time taken for 1 epoch 12.71 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2767\n",
            "Epoch 7 Batch 50 Loss 1.2719\n",
            "Epoch 7 Batch 100 Loss 1.3202\n",
            "Epoch 7 Batch 150 Loss 1.2539\n",
            "\n",
            "Epoch 7 Loss: 1.2902\n",
            "Time taken for 1 epoch 12.55 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2501\n",
            "Epoch 8 Batch 50 Loss 1.2308\n",
            "Epoch 8 Batch 100 Loss 1.2413\n",
            "Epoch 8 Batch 150 Loss 1.2923\n",
            "\n",
            "Epoch 8 Loss: 1.2489\n",
            "Time taken for 1 epoch 11.89 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1967\n",
            "Epoch 9 Batch 50 Loss 1.1864\n",
            "Epoch 9 Batch 100 Loss 1.1872\n",
            "Epoch 9 Batch 150 Loss 1.2088\n",
            "\n",
            "Epoch 9 Loss: 1.2094\n",
            "Time taken for 1 epoch 11.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1387\n",
            "Epoch 10 Batch 50 Loss 1.1575\n",
            "Epoch 10 Batch 100 Loss 1.1599\n",
            "Epoch 10 Batch 150 Loss 1.1819\n",
            "\n",
            "Epoch 10 Loss: 1.1702\n",
            "Time taken for 1 epoch 11.83 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}